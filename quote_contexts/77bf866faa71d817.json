{"created_at": 1766158964.151548, "confirmed": true, "origin": "searched", "search_input": "", "suggested_query": "The multimodal gap thus becomes a site of creative tension, where the very distinction between modalities is what enables the emergence of novel meanings and interpretations. Instead of seeking to eliminate this gap, we can work with it, recognizing that it is precisely this tension that fosters a deeper understanding of the complex relationships between image and text. By acknowledging and embracing the multimodal gap, we can begin to develop new methods for multimodal analysis and generation that take into account the inherent differences between modalities, rather than trying to erase them.", "suggest_context": "He continues by warning that translation : will provide no more than a facsimile, more or less distorted, of our concepts. You have still not cleared a way, built a threshold, for entering. This offers another caution against treating one modality as a field to be converted into another, in our case images and text. Translation in Jullien's terms either leaves the other modality at a distance, or encloses it within our own terms. Either way, one remains outside. Taken together, Foucaults void and Julliens threshold describe the same structural difficulty: whenever two regimes are made commensurable too quickly, what is gained in exchangeability is often paid for in coherence. This stance aligns with the obstacle of the multimodal gap in ML models like CLIP, which operate under assumptions of alignment. What Foucault reveals is that the interplay of image and text resists equivalence; they are not mirrors of one another, but mutually estranging domains. Therefore, the multimodal gap is hypothesised to be not an engineering flaw but an effacement of the 'common place' between the signs of writing and the lines of the image. Figure 2.3:1 - Example of the multimodal gap Machine learning models like CLIP appear to offer a bridge across modalities, embedding images and texts into a shared space, premised on large-scale semantic regularities. They function optimally when translation is direct. CLIP uses a dual-encoder architecture in which distinct encoders process text and image inputs into their respective embedding spaces. These embeddings are then aligned using contrastive loss, which minimizes the cosine distance between matched cross-modal pairs. However, as Liang et al. demonstrate, this optimisation does not eliminate but in fact preserves a distinct modality gap: the embeddings from different modalities consistently occupy separate regions of the shared space. A simple demonstration illustrates this condition. In one experiment, CLIP embeddings for 15637 screenshots from Studio Ghibli films and their corresponding time-stamped subtitles, contextually relevant but non-descriptive pairings, achieved only 0.03 matching accuracy in image-to-text retrieval across the full corpus. Conversely, 456 images from ArchDaily, paired with FUYU-8B generated captions reach 66 accuracy. While this is an expected result given the training criteria of CLIPs embedding space, this shows how CLIPs latent space is tuned not for contextual resonance but for semantic alignment. Figure 2.3:2 - 3D PCA reduced visualisation of ArchDaily and Ghibli image blue and text red pairs. Far from a neutral terrain, the shared latent space is geometrically shaped by the logic of contrastive alignment. Understanding this mechanism opens the door for alternative approaches. Rather than collapsing modalities into a common axis, an architectonic strategy might embrace this separation as a natural condition, one that supports difference, contextual framing, and multimodality."}