{"created_at": 1763995983.7596009, "confirmed": true, "origin": "searched", "search_input": "Both of the multi-modal screens sit within the dimensions corresponding to the multi-modal documents that make up the domain. As an example we will describe the multi-modal structure based on the 216 films described in 3.3. However any other collection of multi-modal documents as described in 3.3 would follow the same process in establishing the 2 operative screens.", "suggested_query": "The use of CLIP word embeddings for figurative screening enables the exploration of multiple facets within the image, allowing for a richer understanding of the overall composition. This is particularly relevant when dealing with images that convey complex ideas or emotions, where a simple descriptive approach may not fully capture the essence of the image. By leveraging the capabilities of CLIP, we can extract a multitude of characteristics and indices from the image, which can then be projected onto the textual counterpart, enabling a deeper understanding of the image's layered content.", "suggest_context": "These considerations lead to the choice of relying on a database format for the generic images, which will be queried based on the associated figurative word-labels. The exact query method will be described in the following chapters. ADD SECTION HERE ON CHOICE OF CLIP WORD EMBEDDINGS FOR FIGURATIVE SCREEN - The descriptive ability to collapse the image and text into one domain relying on the descriptive function provides an essential step of abstraction from the figurative in order for us to be able to deal with the quantity of variety yet consistent content. Alternatives would always have to rely on encodings of other images, but these encodings are of full images, therefore containing within that encoding all aspects of that image. This is great if we want to find one very specific image, or better still to get a description for that particular image. It is also this feature that generative models rely on within the diffusion model to build up a specific thing. But it also means that there is no such thing as an a-specific encoding of an image. Give CLIP the simplest image of an apple, and it is also then encoding the lack of other things as a characteristic. The textual counterpart lets us project a multitude of characteristics and indices onto the image, thus being able to address primary as well as secondary, tertiary etc. layers within the image. This is the reason that the figurative, even when on it's own terms, was encoded through this process. Exactly because it allows us to work with a layered content. - The only other potential way that would be able to engage with this is the use of SAM that lets us disect an image into subsequent parts, which is certainly interesting, but impractical because of heavy to compute, but also in the sense that it is also a very specific and singular manner of looking at the image through it's elements, instead of the whole, where CLIP can also capture multiple facets that are conveyed through the overall composition to a point of course. - So although it might read like we criticise CLIP in chapter 2, it is not a criticism on it as a tool, it is incredible in it's translational capacities. It's just that we'd like to think about the relation between text and image as a richer ground than just that of description, which is why we're eagerly working with CLIP in order to expose these other proportionalities. MULTIMODAL Both of the multi-modal screens sit within the dimensions corresponding to the multi-modal documents that make up the domain. As an example we will describe the multi-modal structure based on the 216 films described in 3.3. However any other collection of multi-modal documents as described in 3.3 would follow the same process in establishing the 2 operative screens."}