{"created_at": 1763855875.9214668, "confirmed": true, "origin": "suggested", "search_input": "While we are able to construct a SOM screen for the figurative words for the films as shown below, this is possible because each film consists of a large series of frames, but doing this with images would be a far too sparse construct. These considerations lead to the choice of relying on a database format for the generic images, which will be queried based on the associated figurative word-labels. The exact query method will be described in the following chapters. ADD SECTION HERE ON CHOICE OF CLIP WORD EMBEDDINGS FOR FIGURATIVE SCREEN - The descriptive ability to collapse the image and text into one domain relying on the descriptive function provides an essential step of abstraction from the figurative in order for us to be able to deal with the quantity of variety yet consistent content. Alternatives would always have to rely on encodings of other images, but these encodings are of full images, therefore containing within that encoding all aspects of that image. This is great if we want to find one very specific image, or better still to get a description for that particular image. It is also this feature that generative models rely on within the diffusion model to build up a specific thing. But it also means that there is no such thing as an a-specific encoding of an image. Give CLIP the simplest image of an apple, and it is also then encoding the lack of other things as a characteristic. The textual counterpart lets us project a multitude of characteristics and indices onto the image, thus being able to address primary as well as secondary, tertiary etc. layers within the image. This is the reason that the figurative, even when on it's own terms, was encoded through this process. Exactly because it allows us to work with a layered content. - The only other potential way that would be able to engage with this is the use of SAM that lets us disect an image into subsequent parts, which is certainly interesting, but impractical because of heavy to compute, but also in the sense that it is also a very specific and singular manner of looking at the image through it's elements, instead of the whole, where CLIP can also capture multiple facets that are conveyed through the overall composition to a point of course. - So although it might read like we criticise CLIP in chapter 2, it is not a criticism on it as a tool, it is incredible in it's translational capacities. It's just that we'd like to think about the relation between text and image as a richer ground than just that of description, which is why we're eagerly working with CLIP in order to expose these other proportionalities. night\n\n*In order to capture these proportionalities, we propose to use a combination of CLIP and a word embedding model, specifically a word2vec model. This will allow us to map the figurative words to their corresponding word embeddings, and then use these embeddings as input to the CLIP model. By doing so, we can capture the contextual relationships between the words and the images, and use these relationships to inform the construction of the SOM screen. This approach will enable us to move beyond a simple descriptive level and capture the more nuanced and complex relationships between the text and images.  Next, we will discuss the choice of word embedding model and how it will be used in combination with CLIP.  We will also describe the process of mapping the figurative words to their corresponding word embeddings and how this will be used to construct the SOM screen.  Finally, we will present the results of the experiment and discuss the implications of the findings.  We believe that this approach will provide a more comprehensive understanding of the relationships between text and images, and will enable us to develop more effective strategies for analyzing and generating figurative language.*", "suggested_query": "In order to capture these proportionalities, we propose to use a combination of CLIP and a word embedding model, specifically a word2vec model. This will allow us to map the figurative words to their corresponding word embeddings, and then use these embeddings as input to the CLIP model. By doing so, we can capture the contextual relationships between the words and the images, and use these relationships to inform the construction of the SOM screen. This approach will enable us to move beyond a simple descriptive level and capture the more nuanced and complex relationships between the text and images.  Next, we will discuss the choice of word embedding model and how it will be used in combination with CLIP.  We will also describe the process of mapping the figurative words to their corresponding word embeddings and how this will be used to construct the SOM screen.  Finally, we will present the results of the experiment and discuss the implications of the findings.  We believe that this approach will provide a more comprehensive understanding of the relationships between text and images, and will enable us to develop more effective strategies for analyzing and generating figurative language.", "suggest_context": "While we are able to construct a SOM screen for the figurative words for the films as shown below, this is possible because each film consists of a large series of frames, but doing this with images would be a far too sparse construct. These considerations lead to the choice of relying on a database format for the generic images, which will be queried based on the associated figurative word-labels. The exact query method will be described in the following chapters. ADD SECTION HERE ON CHOICE OF CLIP WORD EMBEDDINGS FOR FIGURATIVE SCREEN - The descriptive ability to collapse the image and text into one domain relying on the descriptive function provides an essential step of abstraction from the figurative in order for us to be able to deal with the quantity of variety yet consistent content. Alternatives would always have to rely on encodings of other images, but these encodings are of full images, therefore containing within that encoding all aspects of that image. This is great if we want to find one very specific image, or better still to get a description for that particular image. It is also this feature that generative models rely on within the diffusion model to build up a specific thing. But it also means that there is no such thing as an a-specific encoding of an image. Give CLIP the simplest image of an apple, and it is also then encoding the lack of other things as a characteristic. The textual counterpart lets us project a multitude of characteristics and indices onto the image, thus being able to address primary as well as secondary, tertiary etc. layers within the image. This is the reason that the figurative, even when on it's own terms, was encoded through this process. Exactly because it allows us to work with a layered content. - The only other potential way that would be able to engage with this is the use of SAM that lets us disect an image into subsequent parts, which is certainly interesting, but impractical because of heavy to compute, but also in the sense that it is also a very specific and singular manner of looking at the image through it's elements, instead of the whole, where CLIP can also capture multiple facets that are conveyed through the overall composition to a point of course. - So although it might read like we criticise CLIP in chapter 2, it is not a criticism on it as a tool, it is incredible in it's translational capacities. It's just that we'd like to think about the relation between text and image as a richer ground than just that of description, which is why we're eagerly working with CLIP in order to expose these other proportionalities. night"}