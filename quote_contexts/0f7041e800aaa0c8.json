{"created_at": 1763335566.428527, "confirmed": true, "origin": "searched", "search_input": "The subtitles form the documents through which the textual word elements are defined. The subtitles are extracted from the .srt files as separate lines with timestamps. Then the subtitles across each film are combined as one large subtitle document, and after the same preprocessing steps used for the books are applied to all films, a vocabulary of the words used in the films is established. This vocabulary we will refer to as the mm_215_vocab_t .", "suggested_query": "The words within the subtitles, just like those in the paragraphs and the books, will be further encoded using LLM methods, allowing for a more nuanced understanding of the semantic relationships between the words and the subtitles. These word embeddings will be combined with the BGE embeddings from the paragraphs to provide a richer representation of the textual concepts within the multi-modal corpus. The resulting embeddings will be used as input for the multi-modal instrument, enabling it to operate at a more granular level and capture the subtleties of the subtitles.  |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant|> |assistant", "suggest_context": "The TF-IDFLSA approach is sufficient for dealing with books, as it is an appropriate way to operate with themes and overall concept focus through the relative use of vocabulary, but now that we are working with more detailed phrases the nuances that lie in the shorter phrases are more difficult to capture with just these statistical measures. Because the paragraphs are short enough, we now sit at the scale where we gain access to more sophisticated manners of encoding relying on LLM methods, which allows for a more overarching semantic measurement. Specifically we will use BGE embeddings as a second stage encoding. Since computing these is more computationally expensive, this is however only done for smaller numbers of paragraphs. The exact selection and workflow criteria on how these steps are broken down will be described later on in the subsequent chapter of section 3. word The smallest element in the textual domain, and the core symbolic entity by which the multi-modal instrument will operate. It occurs in 3 different ways, leading to each word being possibly represented in three distinct embeddings for 3 different domains. Although the vocabulary is not necessarily identical across these domains so therefore not each word will occur in all three domains. - in context of books: The smallest text entity we will work with is the word on its own. In a way the word vectorisation is the same as the book TF-IDF matrix, just rotated, and normalised differently. However we will not work or rely on this encoding of the words within the 11mvocab. - context of paragraphs: The same goes for the words within the paragraphs. Here the words are also the rotated matrix of the paragraphs. These encodings are used as it allows the search for closest relevant alternative words if a word from the writing input does not occur in the film dictionary. Therefore we do consider the operations on the level of words within the paragraph domain in order to facilitate this. - In a similar fashion to the paragraphs we have extracted all subtitles from the 215 films introduced earlier. Because we're not working with the actual material of the subtitles as sections, but interested in how the subtitles together give shape to the text domain within the multi-modal corpus of films, the subtitles as a whole are not maintained as separate elements which would sit at a similar level to the paragraphs. However the words occurring within the subtitles form the primary elements by which we will speak about the textual concepts within the multi-modal corpus. So where the primary role of words in the other text domains is to form the dimensionality for the documents, here the documents form the dimensions for the words. The vectorisation method of these words will be discussed in the subtitle section later this chapter."}