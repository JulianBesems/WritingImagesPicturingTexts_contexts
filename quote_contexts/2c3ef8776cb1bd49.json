{"created_at": 1763308967.546623, "confirmed": true, "origin": "searched", "search_input": "", "suggested_query": "This process is repeated for all the paragraphs in the subset of books relevant at that moment. The resulting paragraph vectors are stored and can be used for further analysis or as an input for the following steps. In order to make the most out of the gained information, we also implemented a system that allows for the computation of the similarity between paragraphs. This similarity is calculated based on the cosine similarity between the corresponding paragraph vectors. The paragraphs that are most similar to a given paragraph can be used to provide additional context or to identify the most relevant information. The paragraphs can also be clustered based on their similarity, allowing for the identification of topics or themes within the subset of books. With this system, we can efficiently navigate and explore the large corpus of books, providing a deeper understanding of the content and the relationships between the paragraphs.", "suggest_context": "The book as a full entity is in all instances treated purely as text as such. The processing of this from the original file format was discussed in 3.2.1. The next step is to make this full text 'measurable' or in other words create a vectorisation for each book. First the text is pre-processed through filtering on only words, getting rid of all punctuation and capitalisation. Then stop-words are removed, words are lemmatised, and finally non-english words are removed, reducing the full book to a normalised list of terms. A short example of the preprocessing transformation applied to the red quote above: 'see', 'begun', 'see', 'instability', 'meaning', 'uncertainty', 'reference', 'experiencing', 'first', 'hand', 'generally', 'termed', 'linguistic', 'indeterminacy', 'simply', 'function', 'expression', 'interpretation', 'well', 'arises', 'imperfection', 'medium', 'speaker', 'use', 'radical', 'subjectivity', 'listener', 'interpreter', 'reason', 'doubly', 'inescapable', 'condition', 'prevents', 'ever', 'arriving', 'certain', 'complete', 'understanding', 'human', 'affair' Applying this to all 1.1M books yields a total of 63029 unique terms, henceforth referred to as the 11mvocab . Based on the preprocessed terms, a sparse TF-IDF matrix was built, having 1,100,990 rows one per book and 63029 columns one per term. In order to make the dataset more manageable a dimensionality reduction to 300 dimensions was applied to the full matrix through LSA, reducing the matrix to 1,100,990 x 300. paragraph A smaller entity from the books that will play a role is the paragraph. The paragraph is the level at which we will interact most explicitly with the books, and manifest themselves in the thesis as the red and magenta quotes that are included throughout. At the level of the book, it's paragraphs are extracted based on two metrics - first is that the text content is broken up into chunks of text separated by line-breaks. Then these chunks are recursively broken up into what we will refer to as paragraphs based on a maximum length but avoiding the breaking up of sentences, and then adjacent paragraphs that are below a minimum length are combined. As the paragraphs aren't treated in the context of all books but only the subset of books relevant at that moment, the paragraphs are first vectorised in the same manner as the books, but with each paragraph acting as a document amongst all combined paragraphs from all books in the subset. So a TF-IDF matrix and LSA matrix of 300 dimensions is caluclated with each row being a single paragraph. Because the paragraphs are short enough, in addition to the LSA vectorisation we now also gain access to a more sophisticated manner of encoding that allows for a more overarching semantic encoding. also vectorise the paragraphs using the more advanced and modern BGE embedding. However computing these is expensive, so this is only done for smaller numbers of paragraphs. The exact selection and workflow criteria on how these steps are broken down will"}