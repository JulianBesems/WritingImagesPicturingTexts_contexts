{"created_at": 1763316894.9105961, "confirmed": true, "origin": "searched", "search_input": "The smallest text entity we will work with is the word on its own. In a way the word vectorisation is the same as the book or paragraph TF-IDF and LSA matrices, just rotated, and normalised differently. Although this forms a means of working with the books or paragraphs, we will not engage with this within the instrument. The word level is the element that will provide the projections across the domains, as wel will later discuss. For this we do take into consideration the words within the books, as it provides a way of looking for synonyms if words do not occurr in the film dictionary. Therefore we do consider the operations on the level of words within the text domain as well in order to facilitate this.", "suggested_query": "The word vectors are then further processed to generate a concept space that captures the semantic relationships between words, which will be crucial for identifying patterns in the film's dialogue. This concept space is built by applying a word2vec algorithm to the word vectors, resulting in a dense representation of words that can be used for similarity searches. For instance, words that are semantically similar, such as \"actor\" and \"performer\", will have similar vector representations, allowing the instrument to identify and group these words together. This concept space will serve as the foundation for the subsequent steps in the instrument, where we will explore how to map these word vectors to the film's narrative structure.", "suggest_context": "At the level of the book, it's paragraphs are extracted based on two metrics - first is that the text content is broken up into chunks of text separated by line-breaks. Then these chunks are recursively broken up into what we will refer to as paragraphs based on a maximum length but avoiding the breaking up of sentences, and then adjacent paragraphs that are below a minimum length are combined. As the paragraphs aren't treated in the context of all books but only the subset of books relevant at that moment, the paragraphs are first vectorised in the same manner as the books, but with each paragraph acting as a document amongst all combined paragraphs from all books in the subset. So a TF-IDF matrix and LSA matrix of 100 dimensions is caluclated with each row being a single paragraph. For this particular chapter, we are working with 175 books broken down into 153,450 paragraphs, so a resulting LSA matrix of 153,450 x 100. This also illustrates why we're not working with all paragraphs from all books, because if we take the factor 1000 as a rule of thumb, we would end up with a total of around a billion paragraphs in the entire book library. The TF-IDFLSA approach is sufficient for dealing with books, as it is an appropriate way to operate with themes and overall concept focus through the relative use of vocabulary, but now that we are working with more detailed phrases the nuances that lie in the shorter phrases are more difficult to capture with just these statistical measures. Because the paragraphs are short enough, we now sit at the scale where we gain access to more sophisticated manners of encoding relying on LLM methods, which allows for a more overarching semantic measurement. Specifically we will use BGE embeddings as a second stage encoding. Since computing these is more computationally expensive, this is however only done for smaller numbers of paragraphs. The exact selection and workflow criteria on how these steps are broken down will be described later on in the subsequent chapter of section 3. word The smallest text entity we will work with is the word on its own. In a way the word vectorisation is the same as the book or paragraph TF-IDF and LSA matrices, just rotated, and normalised differently. Although this forms a means of working with the books or paragraphs, we will not engage with this within the instrument. The word level is the element that will provide the projections across the domains, as wel will later discuss. For this we do take into consideration the words within the books, as it provides a way of looking for synonyms if words do not occurr in the film dictionary. Therefore we do consider the operations on the level of words within the text domain as well in order to facilitate this."}