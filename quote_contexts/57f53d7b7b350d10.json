{"created_at": 1763320297.087632, "confirmed": true, "origin": "searched", "search_input": "The smallest element in the textual domain, and the core symbolic entity by which the multi-modal instrument will operate. It occurs in 3 different ways, leading to each word being possibly represented in three distinct embeddings for 3 different domains. Although the vocabulary is not necessarily identical across these domains so therefore not each word will occur in all three domains.", "suggested_query": "The words, as the smallest element in the textual domain, are the core symbolic entity by which the multi-modal instrument will operate. Each word, possibly represented in three distinct embeddings for three different domains, can occur in multiple ways leading to three distinct word representations. These word representations are further processed using BGE embeddings as a second stage encoding, which allows for a more overarching semantic measurement. Since computing these is more computationally expensive, this is however only done for smaller numbers of paragraphs. In this way, the multi-modal instrument can effectively capture the nuances that lie in the shorter phrases and the detailed phrases. The paragraphs, now with the word representations embedded in them, are further processed using LLM methods to capture the overarching semantic measurement.", "suggest_context": "In order to make the dataset more manageable a dimensionality reduction to 300 dimensions was applied to the full matrix through LSA, reducing the matrix to 1,100,990 x 300. paragraph A smaller entity from the books that will play a role is the paragraph. The paragraph is the level at which we will interact most explicitly with the books, and manifest themselves in the thesis as the red and magenta quotes that are included throughout. At the level of the book, it's paragraphs are extracted based on two metrics - first is that the text content is broken up into chunks of text separated by line-breaks. Then these chunks are recursively broken up into what we will refer to as paragraphs based on a maximum length but avoiding the breaking up of sentences, and then adjacent paragraphs that are below a minimum length are combined. As the paragraphs aren't treated in the context of all books but only the subset of books relevant at that moment, the paragraphs are first vectorised in the same manner as the books, but with each paragraph acting as a document amongst all combined paragraphs from all books in the subset. So a TF-IDF matrix and LSA matrix of 100 dimensions is caluclated with each row being a single paragraph. For this particular chapter, we are working with 175 books broken down into 153,450 paragraphs, so a resulting LSA matrix of 153,450 x 100. This also illustrates why we're not working with all paragraphs from all books, because if we take the factor 1000 as a rule of thumb, we would end up with a total of around a billion paragraphs in the entire book library. The TF-IDFLSA approach is sufficient for dealing with books, as it is an appropriate way to operate with themes and overall concept focus through the relative use of vocabulary, but now that we are working with more detailed phrases the nuances that lie in the shorter phrases are more difficult to capture with just these statistical measures. Because the paragraphs are short enough, we now sit at the scale where we gain access to more sophisticated manners of encoding relying on LLM methods, which allows for a more overarching semantic measurement. Specifically we will use BGE embeddings as a second stage encoding. Since computing these is more computationally expensive, this is however only done for smaller numbers of paragraphs. The exact selection and workflow criteria on how these steps are broken down will be described later on in the subsequent chapter of section 3. word The smallest element in the textual domain, and the core symbolic entity by which the multi-modal instrument will operate. It occurs in 3 different ways, leading to each word being possibly represented in three distinct embeddings for 3 different domains. Although the vocabulary is not necessarily identical across these domains so therefore not each word will occur in all three domains."}