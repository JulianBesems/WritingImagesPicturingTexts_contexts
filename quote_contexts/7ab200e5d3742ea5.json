{"created_at": 1763135302.574305, "confirmed": true, "origin": "searched", "search_input": "", "suggested_query": "the normalised text domain is further transformed through a set of operations designed to create a uniform structure and facilitate the subsequent multimodal projections. a custom-built script applies a series of tokenisation steps, dividing the text into individual words, punctuation marks, and whitespace characters, and subsequently assigns each token a unique identifier. this allows for the efficient manipulation and querying of the texts, as well as the integration of external data streams. the tokenised texts are then stored in a relational database, where they can be retrieved and recombined in various ways to support the creation of multimodal projections.", "suggest_context": "3.2.1 DATASETS The experimental setup consists of constructing an instrument for multimodal projection, designed to stage conceptual adjacency between modalities without collapsing them into semantic equivalence. Inspired by Miro Roman's exploration of informational architectonics in Play Among Books, this method extends beyond textual constellations to include visual material, rendering image and text as epistemically distinct but interrelated fields, and thus re-situating the multimodal gap as an opportunity for proportion rather than a technical flaw. The selected datasets are comprised of media where image and text are co-present but not inherently descriptive of one another. the GENERICPUBLIC TEXT The generic text domain is conceived as a writable substrate rather than as a library to be read sequentially. it consists of published books and articles gathered from public digital repositories e.g.,Internet Archive Open Library, Project Gutenberg, and comparable sources. After filtering to English and a minimum of 2000 words, a total number of 1,100,990 documents was compiled. All documents were normalised from .pdf or .epub formats to plain UTF-8 .txt. For this the PyMuPDF and EbookLib libraries were relied on. Through this process all other information like page nrs, layout and images are lost. This was done for two reasons, first of all the storage required for 1.1M texts in their original file format was not feasable, thus all documents were immediately converted and stored as .txt files only, still leading to a total combined size of 0.7TB. The other reason is that as an operative instrument of writing and picturing, for the purpose of establishing a public text domain that serves as a generic screen for the multi-modal projections, the other aspects of the books or articles are not required. The intention is not to read any of these documents in a conventional matter, nor retrieve them in full. Instead we are interested in creating a digital substrate, for which the normalisation of data-format forms the first step to facilitate the operability of this TEXT domain. That said, at this stage the texts do remain in legible form, the words are not lemmatised, segmented or filtered. They are merely reduced to their most basic form of plain text, with the only formatting that remains being new-lines. As such these 1.1M texts form the generic domain of text onto which the personal projections in WIPT will be cast. Thereby forming the substrate through which the ability of WIPT to expose precious context from an amount of text no one individual would be able to read. Forming the demonstration of a personal perspective onto a public domain of text."}