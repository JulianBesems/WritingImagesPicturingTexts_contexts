{"created_at": 1763301602.962444, "confirmed": true, "origin": "suggested", "search_input": "3.3 ELEMENTALS Now that we've established the material we'll be using, this chapter will discuss the ways in which the contents are resolved into their subsequent elements. These elements are what will act as the indices by which we will compute the projections and searches within and across the domains. We shall therefore here outline how these elements will act as symbolic representations for their domains, and how this representation is achieved through different ways of encoding the information. TEXT We will first discuss all elements from the text domain that will feature in WIPT. book Even though not all of the documents included in the text domain the 1.1M documents described in 3.2.1 would normally be referred to as books, we will use the term book to refer to any of the highest order documents in the text domain in order to avoid confusion further on with the documents in the multi-modal domains. The book as a full entity is in all instances treated purely as text as such. The processing of this from the original file format was discussed in 3.2.1. The next step is to make this full text 'measurable' or in other words create a vectorisation for each book. First the text is pre-processed through filtering on only words, getting rid of all punctuation and capitalisation. Then stop-words are removed, words are lemmatised, and finally non-english words are removed, reducing the full book to a normalised list of terms. A short example of the preprocessing transformation applied to the red quote above: 'see', 'begun', 'see', 'instability', 'meaning', 'uncertainty', 'reference', 'experiencing', 'first', 'hand', 'generally', 'termed', 'linguistic', 'indeterminacy', 'simply', 'function', 'expression', 'interpretation', 'well', 'arises', 'imperfection', 'medium', 'speaker', 'use', 'radical', 'subjectivity', 'listener', 'interpreter', 'reason', 'doubly', 'inescapable', 'condition', 'prevents', 'ever', 'arriving', 'certain', 'complete', 'understanding', 'human', 'affair' Applying this to all 1.1M books yields a total of 63029 unique terms, henceforth referred to as the 11mvocab . Based on the preprocessed terms, a TF-IDF matrix was built, having 63029 columns one per term and 1,100,990 rows one per document.\n\n*These elements are then used as input to the vector space model, which enables us to perform various operations on the documents such as similarity searches and clustering. Furthermore, the elements serve as the foundation for the construction of the document-topic model, which we will discuss in the next section.*", "suggested_query": "These elements are then used as input to the vector space model, which enables us to perform various operations on the documents such as similarity searches and clustering. Furthermore, the elements serve as the foundation for the construction of the document-topic model, which we will discuss in the next section.", "suggest_context": "3.3 ELEMENTALS Now that we've established the material we'll be using, this chapter will discuss the ways in which the contents are resolved into their subsequent elements. These elements are what will act as the indices by which we will compute the projections and searches within and across the domains. We shall therefore here outline how these elements will act as symbolic representations for their domains, and how this representation is achieved through different ways of encoding the information. TEXT We will first discuss all elements from the text domain that will feature in WIPT. book Even though not all of the documents included in the text domain the 1.1M documents described in 3.2.1 would normally be referred to as books, we will use the term book to refer to any of the highest order documents in the text domain in order to avoid confusion further on with the documents in the multi-modal domains. The book as a full entity is in all instances treated purely as text as such. The processing of this from the original file format was discussed in 3.2.1. The next step is to make this full text 'measurable' or in other words create a vectorisation for each book. First the text is pre-processed through filtering on only words, getting rid of all punctuation and capitalisation. Then stop-words are removed, words are lemmatised, and finally non-english words are removed, reducing the full book to a normalised list of terms. A short example of the preprocessing transformation applied to the red quote above: 'see', 'begun', 'see', 'instability', 'meaning', 'uncertainty', 'reference', 'experiencing', 'first', 'hand', 'generally', 'termed', 'linguistic', 'indeterminacy', 'simply', 'function', 'expression', 'interpretation', 'well', 'arises', 'imperfection', 'medium', 'speaker', 'use', 'radical', 'subjectivity', 'listener', 'interpreter', 'reason', 'doubly', 'inescapable', 'condition', 'prevents', 'ever', 'arriving', 'certain', 'complete', 'understanding', 'human', 'affair' Applying this to all 1.1M books yields a total of 63029 unique terms, henceforth referred to as the 11mvocab . Based on the preprocessed terms, a TF-IDF matrix was built, having 63029 columns one per term and 1,100,990 rows one per document."}