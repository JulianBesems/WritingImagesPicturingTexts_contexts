{"created_at": 1763663401.0644128, "confirmed": true, "origin": "zotero", "search_input": "Applying this to ~1.1M books and retaining words appearing in \u226530 books yields a vocabulary of 63,029 unique terms ( 1_1m_vocab ). We compute a sparse TF\u2011IDF matrix with 1,100,990 rows 63,029 columns (| books | \u00d7 | words |) and reduce it to 300 dimensions via LSA : $|B| * 300$. Because of the size, this was done with truncated SVD in randomised batches. Cosine similarity provides the rotational metric for operations over $B$.", "suggested_query": "Within the vectorial face of a book, we can identify a set of words that appear frequently, which we term the 'core vocabulary'. This core vocabulary can be used to represent the book in a low-dimensional space, facilitating its comparison to other books. We further explore the properties of this representation, including its stability under perturbations and its ability to capture subtle semantic relationships between books.", "suggest_context": "- writing :x - current editor text, encoded as a paragraph-like vector - suggestion :sigmax: suggestion - LLM-extrapolated text from x encoded in as a paragraph-like vector There is also a number of elements that do not have a vecorial face, but are rather inferred through the information of others: - subtitle - a line of subtitle in a film that contains a number of words and also a timestamp ts - scene - a continuous piece of film inferred through the aggregation of adjacent frames maintaining 30 label continuity in fw around a seed frame. We distinguish 3 textual word embeddings: wtB book-scale, wtp paragraph-scale, wtF subtitle-scale and 2 image word embeddings wf public image scale, and wfF film frame scale. Within the textual or figurative, the informational identity of the word can serve as a bridge between vectorial faces, however across domains the informational identity is never assumed to be equilavent; a word names different anchors in different domains. TEXT book We use book to refer to any highest-order document in the text domain, even if some sources e.g., articles are not books in the strict sense. The book as a full entity is in all instances treated purely as text as such. The processing from the original file format was discussed in 3.2.1. Next is to make this full text 'measurable' or in other words create a vectorisation for each book. To preprocess the contents we filter to words, lowercase, strip punctuation, remove stopwords, lemmatise, and remove nonEnglish terms. For example, the red quote above becomes the following list of terms: 'see', 'begun', 'see', 'instability', 'meaning', 'uncertainty', 'reference', 'experiencing', 'first', 'hand', 'generally', 'termed', 'linguistic', 'indeterminacy', 'simply', 'function', 'expression', 'interpretation', 'well', 'arises', 'imperfection', 'medium', 'speaker', 'use', 'radical', 'subjectivity', 'listener', 'interpreter', 'reason', 'doubly', 'inescapable', 'condition', 'prevents', 'ever', 'arriving', 'certain', 'complete', 'understanding', 'human', 'affair' Applying this to 1.1M books and retaining words appearing in 30 books yields a vocabulary of 63,029 unique terms  11mvocab . We compute a sparse TFIDF matrix with 1,100,990 rows 63,029 columns  books    words  and reduce it to 300 dimensions via LSA : B  300. Because of the size, this was done with truncated SVD in randomised batches. Cosine similarity provides the rotational metric for operations over B."}