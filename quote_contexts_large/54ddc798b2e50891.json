{"created_at": 1763135253.3494961, "confirmed": true, "origin": "zotero", "search_input": "", "suggested_query": "The normalised texts were then processed using a combination of natural language processing techniques and custom scripts to extract relevant features and metadata, such as part-of-speech tags, named entity recognition, and sentence structure. These features were used to create a matrix of semantic relationships between the texts, which could be visualized as a network graph to facilitate the identification of conceptual clusters and patterns of association. The resulting dataset comprised over 2.5 million entries, with each entry representing a unique semantic connection between two or more texts.", "suggest_context": "3.2.1 DATASETS The experimental setup consists of constructing an instrument for multimodal projection, designed to stage conceptual adjacency between modalities without collapsing them into semantic equivalence. Inspired by Miro Roman's exploration of informational architectonics in Play Among Books, this method extends beyond textual constellations to include visual material, rendering image and text as epistemically distinct but interrelated fields, and thus re-situating the multimodal gap as an opportunity for proportion rather than a technical flaw. The selected datasets are comprised of media where image and text are co-present but not inherently descriptive of one another. the GENERICPUBLIC TEXT The generic text domain is conceived as a writable substrate rather than as a library to be read sequentially. it consists of published books and articles gathered from public digital repositories e.g.,Internet Archive Open Library, Project Gutenberg, and comparable sources. After filtering to English and a minimum of 2000 words, a total number of 1,100,990 documents was compiled. All documents were normalised from .pdf or .epub formats to plain UTF-8 .txt. For this the PyMuPDF and EbookLib libraries were relied on. Through this process all other information like page nrs, layout and images are lost. This was done for two reasons, first of all the storage required for 1.1M texts in their original file format was not feasable, thus all documents were immediately converted and stored as .txt files only, still leading to a total combined size of 0.7TB. The other reason is that as an operative instrument of writing and picturing, for the purpose of establishing a public text domain that serves as a generic screen for the multi-modal projections, the other aspects of the books or articles are not required. The intention is not to read any of these documents in a conventional matter, nor retrieve them in full. Instead we are interested in creating a digital substrate, for which the normalisation of data-format forms the first step to facilitate the operability of this TEXT domain. That said, at this stage the texts do remain in legible form, the words are not lemmatised, segmented or filtered. They are merely reduced to their most basic form of plain text, with the only formatting that remains being new-lines."}