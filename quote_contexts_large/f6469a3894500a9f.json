{"created_at": 1763297764.8990688, "confirmed": true, "origin": "suggested", "search_input": "3.3 ELEMENTALS Now that we've established the material we'll be using, this chapter will discuss the ways in which the contents are resolved into their subsequent elements. These elements are what will act as the indices by which we will compute the projections and searches within and across the domains. We shall therefore here outline how these elements will act as symbolic representations for their domains, and how this representation is achieved through different ways of encoding the information. TEXT We will first discuss all elements from the text domain that will feature in WIPT. book Even though not all of the documents included in the text domain the 1.1M documents described in 3.2.1 would normally be referred to as books, we will use the term book to refer to any of the highest order documents in the text domain in order to avoid confusion further on with the documents in the multi-modal domains. The book as a full entity is in all instances treated purely as text as such. The processing of this from the original file format was discussed in 3.2.1. The next step is to make this full text 'measurable' or in other words create a vectorisation for each book. Given the scale and length of each book, the method used for this is TF-IDF with LSA applied afterwards. Before building the TF-IDF matrix the text is pre-processed through first finding all words, getting rid of all punctuation and capitalisation. Then stop-words are removed, words are lemmatised, and finally non-english words are removed, reducing the full book to a normalised list of terms. A short example of the preprocessing transformation applied to the red quote above: 'see', 'begun', 'see', 'instability', 'meaning', 'uncertainty', 'reference', 'experiencing', 'first', 'hand', 'generally', 'termed', 'linguistic', 'indeterminacy', 'simply', 'function', 'expression', 'interpretation', 'well', 'arises', 'imper\n\n*We will then discuss all elements from the image domain that will feature in WIPT, including the 1.2M images described in 3.2.2. The next step is to make these images 'measurable' or in other words create a vectorisation for each image. Given the scale and complexity of each image, the method used for this is CNN with LSA applied afterwards. Before building the CNN matrix the image is pre-processed through first detecting all objects, getting rid of all noise and distortion. Then edge detection and colour segmentation are applied, followed by object recognition and feature extraction. Finally, the full image is reduced to a normalised list of features. A short example of the preprocessing transformation applied to the blue image above:  ...*", "suggested_query": "We will then discuss all elements from the image domain that will feature in WIPT, including the 1.2M images described in 3.2.2. The next step is to make these images 'measurable' or in other words create a vectorisation for each image. Given the scale and complexity of each image, the method used for this is CNN with LSA applied afterwards. Before building the CNN matrix the image is pre-processed through first detecting all objects, getting rid of all noise and distortion. Then edge detection and colour segmentation are applied, followed by object recognition and feature extraction. Finally, the full image is reduced to a normalised list of features. A short example of the preprocessing transformation applied to the blue image above:  ...", "suggest_context": "3.3 ELEMENTALS Now that we've established the material we'll be using, this chapter will discuss the ways in which the contents are resolved into their subsequent elements. These elements are what will act as the indices by which we will compute the projections and searches within and across the domains. We shall therefore here outline how these elements will act as symbolic representations for their domains, and how this representation is achieved through different ways of encoding the information. TEXT We will first discuss all elements from the text domain that will feature in WIPT. book Even though not all of the documents included in the text domain the 1.1M documents described in 3.2.1 would normally be referred to as books, we will use the term book to refer to any of the highest order documents in the text domain in order to avoid confusion further on with the documents in the multi-modal domains. The book as a full entity is in all instances treated purely as text as such. The processing of this from the original file format was discussed in 3.2.1. The next step is to make this full text 'measurable' or in other words create a vectorisation for each book. Given the scale and length of each book, the method used for this is TF-IDF with LSA applied afterwards. Before building the TF-IDF matrix the text is pre-processed through first finding all words, getting rid of all punctuation and capitalisation. Then stop-words are removed, words are lemmatised, and finally non-english words are removed, reducing the full book to a normalised list of terms. A short example of the preprocessing transformation applied to the red quote above: 'see', 'begun', 'see', 'instability', 'meaning', 'uncertainty', 'reference', 'experiencing', 'first', 'hand', 'generally', 'termed', 'linguistic', 'indeterminacy', 'simply', 'function', 'expression', 'interpretation', 'well', 'arises', 'imper"}